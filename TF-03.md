# TF-03: The Model (Formulation)

Any persisting agent maintains a **model** — a compressed representation of its interaction history that supports prediction and action selection.

## Definitions

**Interaction History** ($\mathcal{H}_t$): The complete record of observations and actions available to the agent:

*[Definition]*
$$\mathcal{H}_t = (o_1, a_1, o_2, a_2, \ldots, a_{t-1}, o_t)$$

This is the agent's *only* raw material. Everything the agent "knows" must be constructed from this.

**Model** ($M_t$): A compression of the interaction history:

*[Definition]*
$$M_t = \phi(\mathcal{H}_t)$$

where $\phi: \mathcal{H}^* \to \mathcal{M}$ maps histories to elements of a **model space** $\mathcal{M}$.

**Model Space** ($\mathcal{M}$): The set of representable models. This is a structural choice that constrains what the agent can represent:

| Model Space $\mathcal{M}$ | Instance |
|----------------------------|----------|
| $\mathbb{R}^n \times \mathbb{S}^n_{++}$ (vector + covariance) | Kalman filter |
| $\Delta(\Theta)$ (distributions over parameters) | Bayesian agent |
| $\{Q: \mathcal{S} \times \mathcal{A} \to \mathbb{R}\}$ (value functions) | RL agent |
| Neural network weight space | Deep learning agent |
| $\mathbb{R}^3$ (error, integral, derivative) | PID controller |
| (Procedures, beliefs, culture) | Organization |
| Antibody repertoire + memory cells | Immune system |

## The Sufficient Statistic Property

A model is **adequate** to the degree that it is a sufficient statistic for the history with respect to future prediction. Formally, define **model sufficiency**:

*[Definition (sufficiency)]*
$$S(M_t) = 1 - \frac{I(\mathcal{H}_t \,;\, o_{t+1:\infty} \mid M_t, a_{t:\infty})}{I(\mathcal{H}_t \,;\, o_{t+1:\infty} \mid a_{t:\infty})}$$

When $S = 1$: the model captures everything in the history that is predictively relevant. Knowing $\mathcal{H}_t$ in addition to $M_t$ provides no further predictive power.

When $S = 0$: the model captures nothing predictively relevant.

Perfect sufficiency ($S = 1$) is an ideal. Real models are approximately sufficient, and the degree of sufficiency is itself a measure of model quality.

**Convention when denominator vanishes.** If the environment is memoryless (pure noise), the history carries no predictive information: $I(\mathcal{H}_t; o_{t+1:\infty} \mid a_{t:\infty}) = 0$, making the denominator zero. In this case, define $S(M_t) \equiv 1$: any model — including an empty one — is trivially sufficient because there is nothing predictively relevant to compress.

**Policy-conditioned sufficiency (operational form).** The definition above conditions on the full future action sequence $a_{t:\infty}$, making $S$ a property of the model's *representational capacity* independent of the agent's policy. This is the theoretical ideal but is non-operational: it cannot be computed or estimated from finite data. For all practical purposes — empirical estimation, optimization targets, falsifiable predictions — use the **policy-conditioned** form:

*[Definition (Operational Form)]*
$$S_\Pi(M_t) = 1 - \frac{I(\mathcal{H}_t \,;\, o_{t+1:t+T_h} \mid M_t, \pi \in \Pi)}{I(\mathcal{H}_t \,;\, o_{t+1:t+T_h} \mid \pi \in \Pi)}$$

where $T_h$ is a finite prediction horizon and $\Pi$ is the policy class under consideration. This is the measured object in any empirical application of TFT. It answers a more specific question: "is $M_t$ a sufficient statistic for prediction *under policies the agent might actually use* over a *finite horizon*?" A model may have $S_\Pi \approx 1$ for one policy class but $S_{\Pi'} < 1$ for another — a chess engine's model is sufficient for its own search policy but not for an alien evaluation function.

The full-sequence $S(M_t)$ remains the theoretical upper bound: $S(M_t) \geq S_\Pi(M_t)$ for any $\Pi$ and any $T_h$. Throughout TFT, results that depend on sufficiency (TF-05's mismatch inevitability, TF-10's structural adaptation trigger) hold for both forms — they require only that the model falls short of perfect sufficiency, which the operational form can detect.

## The Recursive Property

When the model is a sufficient statistic ($S \approx 1$), a critical property follows: the model can be **updated recursively** without reprocessing the full history:

*[Derived (recursive-update)]*
$$M_t = f(M_{t-1}, o_t, a_{t-1})$$

This is not merely convenient — it is what makes real-time adaptation physically possible. An agent that must reprocess its entire history on each observation would face unbounded computational cost as history grows.

The function $f$ is the **update function** — the mechanism by which new information is incorporated into the model. Its properties are the subject of TF-05 and TF-06.

## Model Quality as Compression Efficiency

The model faces a fundamental trade-off formalized by the **information bottleneck**:

*[Formulation (IB-objective)]*
$$\phi^* = \arg\min_{\phi} \left[ I(M_t; \mathcal{H}_t) - \beta \cdot I(M_t; o_{t+1:\infty} \mid a_{t:\infty}) \right]$$

- $I(M_t; \mathcal{H}_t)$ measures **compression cost** — how much of the history the model retains
- $I(M_t; o_{t+1:\infty} \mid a_{t:\infty})$ measures **predictive power** — how much the model can predict
- $\beta$ controls the trade-off: higher $\beta$ favors prediction; lower favors compression

Varying $\beta$ traces a Pareto frontier of optimal models. This IS the rate-distortion curve for the agent's epistemic problem.

A PID controller's model ($\mathbb{R}^3$) is an aggressively compressed point on this curve — it retains almost nothing of the history but preserves enough for effective control of simple dynamics. A Bayesian posterior retains much more. Neither is universally superior; the optimal point depends on $\mathcal{M}$, the environment's complexity, and the agent's computational constraints.

**Connection to environmental volatility.** The optimal $\beta$ is not static — it depends on the environment's rate of change $\rho$ (TF-11). In highly volatile environments (high $\rho$), the predictive value of historical detail decays rapidly: detailed memories of past dynamics become stale as the environment shifts. The optimal $\beta$ is therefore *lower* — the agent should compress aggressively, retaining only the most robust predictive structure, because (a) historical detail has diminishing predictive shelf-life, and (b) complex models with many parameters have lower effective $\eta^*$ per parameter (TF-06), reducing adaptive tempo $\mathcal{T}$. In stable environments (low $\rho$), $\beta$ can be higher: the agent has the luxury of retaining dense historical specificities for nuanced, high-fidelity predictions. This connects TF-03's representation choice to TF-11's persistence condition: an agent that retains too much history in a volatile environment pays twice — once in computational cost, and once in reduced $\mathcal{T}$ that may fall below the persistence threshold.

## Model Class Fitness

The choice of $\mathcal{M}$ constrains what models are representable. If $\mathcal{M}$ cannot contain any good sufficient statistic for the actual environment dynamics, no amount of parameter updating within $\mathcal{M}$ will produce an adequate model.

Define **model class fitness**:

*[Definition (model-class-fitness)]*
$$\mathcal{F}(\mathcal{M}) = \sup_{M \in \mathcal{M}} S(M)$$

The best achievable sufficiency given the model class.

**Note on implicit environment dependence.** The notation $\mathcal{F}(\mathcal{M})$ is a deliberate simplification. Since $S(M)$ depends on the joint distribution of histories and future observations — which is determined by the environment dynamics $(T, h)$ — model class fitness is properly a joint property: $\mathcal{F}(\mathcal{M}; T, h)$. We suppress this dependence for readability, but the implicit conditioning matters: when the environment changes, $\mathcal{F}(\mathcal{M})$ changes even though $\mathcal{M}$ does not. This is precisely what triggers structural adaptation (TF-10) — not that the model class has deteriorated in any absolute sense, but that the environment has moved to a regime where $\mathcal{F}(\mathcal{M}; T', h')$ is lower than it was under the prior dynamics.

When $\mathcal{F}(\mathcal{M})$ is poor, the agent requires **structural adaptation** — changing $\mathcal{M}$ itself. This is the subject of TF-10.

## Why This Is a Formulation

This document is labeled "Formulation" rather than "Axiom" because it makes a specific modeling choice: we analyze adaptive systems as maintaining compressed predictive representations. This is a representational definition, not a psychological or metaphysical claim — "model" includes any internal state that induces non-random action-history dependence, from a thermostat's bimetallic strip to a Bayesian posterior.

The framing is deliberately broad. An agent whose actions are non-random with respect to outcomes is, by definition, using some function of its history — i.e., a model, however implicit. This makes the formulation nearly tautological within the theory's scope, which is by design: TFT does not claim that all systems "have models" in any deep sense, but rather that analyzing them as maintaining compressed representations is productive when the formal apparatus (sufficiency, information bottleneck, model class fitness) can be meaningfully applied.

The model may be explicit (a Kalman state vector, a Bayesian posterior) or implicit (a subsumption architecture's wiring, a PID controller's three-number state, an organization's culture). The formulation claims only that it *exists*, not that it takes any particular form. The theory's content comes not from this existence claim but from the specific mathematical framework ($S(M_t)$, the information bottleneck, $\mathcal{F}(\mathcal{M})$) applied to it.
