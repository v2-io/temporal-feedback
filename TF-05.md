# TF-05: The Update Gain (Derived + Empirical Claim)

The model incorporates mismatch signals through an **update gain** that determines how much each observation shifts the model. The optimal gain balances model uncertainty against observation uncertainty.

## The Update Rule

In its general additive form:

$$M_t = M_{t-1} + \eta(M_{t-1}, o_t) \cdot g(\delta_t)$$

where:
- $\eta$: the **update gain** — a scalar, vector, or matrix determining how much to adjust
- $g$: the **mismatch transform** — a function mapping raw mismatch to the appropriate adjustment direction (identity in the linear case; potentially nonlinear)

In the event-driven formulation (TF-03), this becomes channel-specific:

$$M_{\tau^+} = M_{\tau^-} + \eta^{(k)}(M_{\tau^-}, o^{(k)}_\tau) \cdot g^{(k)}(\delta^{(k)}_\tau)$$

## The Optimal Gain

**Claim** (Empirical — validated exactly in some domains, approximately in others):

The optimal update gain has the structural form:

$$\eta^* = \frac{U_M}{U_M + U_o}$$

where $U_M$ is the **model's uncertainty** about the quantity being observed and $U_o$ is the **observation channel's uncertainty** (noise, unreliability).

### Interpretation

- When $U_M \gg U_o$ (model is uncertain, observations reliable): $\eta^* \to 1$. Trust the observation; shift the model substantially.
- When $U_M \ll U_o$ (model is confident, observations noisy): $\eta^* \to 0$. Trust the model; discount the observation.
- When $U_M \approx U_o$: $\eta^* \approx 0.5$. Weight model and observation equally.

This is the **uncertainty ratio principle**: new information should be weighted in proportion to the relative reliability of the source versus the existing model.

## Domain Validation

**Kalman filter (exact):**

$$K_t = P_{t|t-1} H^T (H P_{t|t-1} H^T + R)^{-1}$$

In the scalar case: $K_t = \frac{p_{t|t-1}}{p_{t|t-1} + r}$ where $p$ is state variance (model uncertainty) and $r$ is measurement noise variance (observation uncertainty). This is exactly $\frac{U_M}{U_M + U_o}$. The matrix form is the natural generalization to multiple dimensions.

**Status: Exact instantiation.** ✓

**Bayesian posterior (exact in conjugate families):**

For an exponential family with conjugate prior of effective sample size $\kappa$ (prior strength), after observing $n$ data points:

$$\theta_{\text{posterior}} = \frac{n}{n + \kappa} \cdot \bar{\theta}_{\text{data}} + \frac{\kappa}{n + \kappa} \cdot \theta_{\text{prior}}$$

The weight on new data, $\frac{n}{n+\kappa}$, increases as data accumulates (model uncertainty decreases relative to observation information). The weight on the prior, $\frac{\kappa}{n+\kappa}$, reflects the prior's strength. This IS the uncertainty ratio.

**Status: Exact instantiation** (for conjugate families). ✓

**Reinforcement learning (approximate):**

Standard TD update: $Q(s,a) \leftarrow Q(s,a) + \alpha[\delta_t]$

The fixed learning rate $\alpha$ is a **degenerate gain** — not adapted to uncertainty. This is a known limitation. Methods that approximate the optimal gain:
- **Bayesian RL**: maintains posterior over $Q$; update weight adapts to uncertainty
- **UCB / Thompson sampling**: exploration bonus proportional to uncertainty
- **Adaptive optimizers** (Adam, RMSProp): approximate second-order information to adapt step sizes per-parameter

The fixed-$\alpha$ case can be understood as a *constant approximation* of the optimal gain — effective when the uncertainty ratio is roughly stable, suboptimal when it varies.

**Status: Approximate instantiation.** The structure holds; basic RL uses a degenerate form; advanced methods converge toward the optimal form. ✓

**PID control (loose mapping):**

PID gains $(K_p, K_i, K_d)$ are typically fixed at design time. Auto-tuning methods (Ziegler-Nichols, relay feedback) estimate plant characteristics to set gains — effectively estimating the uncertainty ratio once at calibration. Adaptive PID and Model Predictive Control (MPC) adjust gains online, moving toward the full adaptive framework.

**Status: Simplified instantiation.** PID fixes $\eta$ at design time; MPC is the richer control-theoretic instantiation. ✓

## Gain Dynamics

The optimal gain changes over time following predictable patterns:

**Convergence**: As the model accumulates information, $U_M$ decreases, so $\eta^* \to 0$. The model becomes increasingly resistant to individual observations. This IS:
- Kalman filter convergence ($P_t \to$ steady state $\Rightarrow K_t \to$ steady state)
- Bayesian posterior concentration
- RL learning rate annealing

**Reset after structural change**: When the environment changes in ways the model cannot track incrementally ($\mathcal{M}$ is wrong; see TF-07), $U_M$ should spike — the model "admits" its uncertainty. The gain increases, enabling rapid re-learning. This is:
- Kalman filter with adaptive process noise
- Bayesian changepoint detection
- RL with learned learning rates

An agent whose gain does NOT reset after structural change will continue trusting a stale model — the analog of Boyd's "incestuous amplification" and the cause of brittle failure in non-stationary environments.

## Overfitting as Gain Miscalibration

From TF-04's decomposition:

$$\mathbb{E}[|\delta_t|^2] = \text{model error}^2 + \text{irreducible noise}^2$$

An agent with $\eta$ too high will adjust its model to explain observation noise, increasing model error on future predictions. An agent with $\eta$ too low will fail to correct genuine model errors.

The optimal gain implicitly separates signal from noise by weighting observations in proportion to their informativeness — exactly what $U_M/(U_M + U_o)$ achieves when $U_o$ includes the irreducible noise.

## Open Questions

1. **Non-parametric models**: For neural networks and other models without a well-defined scalar "model uncertainty," how should $U_M$ be computed? Ensemble methods, dropout-based uncertainty, and Bayesian neural networks are all approximations. Is there a principled general definition? (*Confidence this can be resolved: ~50%*)

2. **Matrix vs. scalar gain**: In high-dimensional systems, the gain is a matrix (Kalman) or per-parameter (Adam), not a scalar. The uncertainty ratio principle holds per-dimension, but the cross-dimensional structure (covariance) adds complexity. Is the scalar formulation a useful simplification or does it lose essential structure? (*The scalar captures the principle; the matrix captures the full optimization. Both are valid at their level of description.*)
