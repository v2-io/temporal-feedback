# TF-05: The Mismatch Signal (Derived)

The discrepancy between model prediction and actual observation — the **mismatch signal** — is the fundamental driver of model adaptation.

## Definition

Given model $M_{t-1}$ and prior action $a_{t-1}$, the model generates a **prediction** of the next observation:

$$\hat{o}_t = \mathbb{E}[o_t \mid M_{t-1}, a_{t-1}]$$

or, more generally, a predictive distribution $P(o_t \mid M_{t-1}, a_{t-1})$.

The **mismatch signal**:

$$\delta_t = o_t - \hat{o}_t$$

In the distributional case, this generalizes to the score function:

$$\delta_t = -\nabla_M \log P(o_t \mid M_{t-1}, a_{t-1})$$

which points in the direction of steepest increase in the likelihood of the actual observation under the model — i.e., the direction the model should move.

## Proposition 5.1: Mismatch Inevitability

**Statement.** For any agent-environment pair in $\mathcal{S}_{\text{TFT}}$ (TF-01), if the agent maintains a model $M_t \in \mathcal{M}$ with $S(M_t) < 1$ (TF-03) — i.e., the model is not a perfect sufficient statistic — then the expected squared mismatch is strictly positive:

$$\mathbb{E}[|\delta_t|^2] > 0$$

**Assumptions.** At least one of the following holds: (a) the observation noise $\varepsilon_t$ in TF-01 is non-degenerate ($\text{Var}(o_t \mid \Omega_t) > 0$), or (b) the model class is strictly insufficient ($S(M_t) < 1$, so the model's predictions differ from the true conditional distribution on a set of positive measure). In practice, both hold simultaneously within $\mathcal{S}_{\text{TFT}}$.

**Proof sketch.**

1. By TF-01, $H(\Omega_t \mid \mathcal{H}_t) > 0$ — the environment retains residual uncertainty even given the full interaction history.
2. By TF-03, the model generates predictions $\hat{o}_t = \mathbb{E}[o_t \mid M_{t-1}, a_{t-1}]$.
3. Decompose the mismatch into model error and noise. Let $\bar{o}_t = \mathbb{E}[o_t \mid \Omega_t]$ be the true conditional mean. Then:

$$\mathbb{E}[|\delta_t|^2] = \mathbb{E}[|o_t - \hat{o}_t|^2] = \underbrace{\mathbb{E}[|\hat{o}_t - \bar{o}_t|^2]}_{\text{(i) model prediction error}} + \underbrace{\mathbb{E}[\text{Var}(o_t \mid \Omega_t)]}_{\text{(ii) observation noise}}$$

   (This follows from the law of total variance: $o_t - \hat{o}_t = (\bar{o}_t - \hat{o}_t) + (o_t - \bar{o}_t)$, and the cross-term vanishes because $o_t - \bar{o}_t$ is mean-zero given $\Omega_t$ and independent of $\hat{o}_t - \bar{o}_t$ which is $\Omega_t$-measurable.)

4. **Term (ii):** Under assumption (a), $\mathbb{E}[\text{Var}(o_t \mid \Omega_t)] > 0$, which alone gives $\mathbb{E}[|\delta_t|^2] > 0$.
5. **Term (i):** Under assumption (b), $S(M_t) < 1$ means $M_t$ loses information about $\Omega_t$ relative to $\mathcal{H}_t$. By the data processing inequality, this information loss implies $\hat{o}_t \neq \bar{o}_t$ on a set of positive probability over $\Omega_t$, so $\mathbb{E}[|\hat{o}_t - \bar{o}_t|^2] > 0$.
6. Either source of positivity suffices; both hold simultaneously in typical settings. $\square$

**Corollary.** Mismatch signals are *inevitable* within the scope of TFT. They can be *reduced* (by improving $S(M_t)$, which shrinks term (i)) but not *eliminated* (because observation noise contributes term (ii), and residual model inadequacy contributes term (i)). The mismatch signal is therefore a permanent feature of any adaptive agent, not a transient artifact of a poor model.

## Why This Is Derived, Not Axiomatic

The proposition above makes precise what was previously stated informally: the mismatch signal follows from TF-03 (the model generates predictions from compressed history) and TF-01 (observations arrive from an uncertain environment). Given any model that predicts and any observation that arrives, their difference exists. The mismatch signal is not an additional assumption but a *consequence* of having a predictive model in an uncertain world.

## Properties

**Vanishing mismatch**: If the model perfectly predicts observations, $\delta_t = 0$ for all $t$. The model needs no updating (but see the ambiguity warning below).

**Decomposition**: The expected squared mismatch decomposes as (from the proof of Proposition 5.1):

$$\mathbb{E}[|\delta_t|^2] = \underbrace{\mathbb{E}[|\hat{o}_t - \bar{o}_t|^2]}_{\text{model error (reducible)}} + \underbrace{\mathbb{E}[\text{Var}(o_t \mid \Omega_t)]}_{\text{observation noise (irreducible)}}$$

where $\bar{o}_t = \mathbb{E}[o_t \mid \Omega_t]$. The model can only drive the first term toward zero. The second term — **aleatoric uncertainty** — is a property of the observation channel, not the model. An agent that tries to eliminate all mismatch (including irreducible noise) will **overfit**: adjusting its model to explain noise, degrading future predictions.

**Information content**: The mismatch signal carries information about the gap between model and reality:

$$I(\delta_t; \Omega_t \mid M_{t-1}) > 0 \quad \text{when the model is imperfect}$$

An agent that ignores mismatch signals loses information. An agent that overreacts to them treats noise as signal. The optimal response — the update gain — is the subject of TF-06.

## The Zero-Mismatch Ambiguity

$\delta_t \approx 0$ does **not** necessarily indicate model adequacy. It indicates that predictions match observations, which may occur because:

**(a) Accurate model.** The model genuinely reflects reality. (*Desirable.*)

**(b) Insufficient exploration.** The agent is only observing aspects of the environment that its model already explains, while remaining ignorant of aspects where the model is wrong. (*Dangerous — confirmation bias.*)

**(c) Low channel resolution.** The observation channel is too noisy or low-bandwidth to detect model errors. (*Architectural limitation.*)

Only (a) is desirable. Cases (b) and (c) represent situations where the mismatch signal is **uninformative** — not because the model is good, but because the agent isn't generating observations that would *test* the model.

This ambiguity is why **active testing** — choosing actions specifically to generate informative mismatch signals — can be valuable for agents that have the capacity to vary their actions (TF-07 or TF-08). An agent that only exploits its model (acting to maximize predicted reward) will tend toward case (b). An agent that explores (acting to maximize expected information from $\delta$) will discover model errors faster. When reliable external models are accessible (other agents, reference sources), **query actions** (TF-07 or TF-08) can resolve this ambiguity far more efficiently than direct probing — the external model's response effectively tests the agent's model against a different, potentially superior compression of the same environment.

## Domain Instantiations

| Domain | Mismatch signal $\delta_t$ |
|--------|---------------------------|
| Kalman filter | Innovation: $\tilde{y}_t = y_t - H\hat{x}_{t \mid t-1}$ |
| RL (TD learning) | TD error: $r_t + \gamma \max_{a'} Q(s', a') - Q(s_t, a_t)$ |
| Bayesian inference | Surprise: $-\log P(D_t \mid \theta_{\text{MAP}})$ |
| PID control | Error: $e_t = r_t - y_t$ (setpoint − measurement) |
| Active inference | Free energy gradient: $\nabla_\mu F$ |
| Scientific method | Experimental anomaly: observation contradicting theory |
| Boyd's OODA | Disorientation: outcomes contradicting mental model |
| Immune system | Unrecognized antigen: pathogen not matching antibody repertoire |
