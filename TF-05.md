# TF-05: The Mismatch Signal (Derived)

The discrepancy between model prediction and actual observation — the **mismatch signal** — is the fundamental driver of model adaptation.

## Definition

Given model $M_{t-1}$ and prior action $a_{t-1}$, the model generates a **prediction** of the next observation:

*[Definition]*
$$\hat{o}_t = \mathbb{E}[o_t \mid M_{t-1}, a_{t-1}]$$

or, more generally, a predictive distribution $P(o_t \mid M_{t-1}, a_{t-1})$.

The **mismatch signal**:

*[Definition]*
$$\delta_t = o_t - \hat{o}_t$$

In the distributional case, this generalizes to the score function:

*[Definition (Distributional Generalization)]*
$$\delta_t = -\nabla_M \log P(o_t \mid M_{t-1}, a_{t-1})$$

which points in the direction of steepest increase in the likelihood of the actual observation under the model — i.e., the direction the model should move.

**Note on primary vs. generalized definition.** The prediction error $o_t - \hat{o}_t$ is the primary definition used in the mismatch dynamics (TF-11, Appendix A) and in the decomposition of Proposition 5.1 below. The score function is its information-theoretic generalization, appropriate when the observation space is not a vector space (e.g., categorical outcomes) or when the model's predictive distribution is the natural object. The two coincide up to scaling under Gaussian models. When TF-06's update rule writes $M_t = M_{t-1} + \eta \cdot g(\delta_t)$, the transform $g$ maps from whichever space $\delta_t$ lives in to the model's update space: if $\delta_t$ is a prediction error in $\mathcal{O}$, then $g$ includes the observation-to-model mapping; if $\delta_t$ is already a score in $T_M\mathcal{M}$, then $g$ may be an identity or a metric adjustment (e.g., the inverse Fisher information for natural gradient).

**Note on units and interpretation.** These two definitions of $\delta_t$ live in different spaces: the prediction error $o_t - \hat{o}_t$ is in observation space $\mathcal{O}$, while the score function $-\nabla_M \log P$ is in the tangent space of the model space $\mathcal{M}$. For the mismatch dynamics in TF-11 (which use $\|\delta\|$ as a scalar magnitude), we adopt the information-theoretic interpretation: $\|\delta\|$ measures the magnitude of the agent's "surprise" — formally, the deviation of the observation from the model's prediction as measured by the negative log-likelihood or an equivalent information-theoretic divergence. Under Gaussian models, the squared prediction error is proportional to the negative log-likelihood, so the two definitions are monotonically related and the dynamics are equivalent up to scaling. For non-Gaussian models, the score function is the natural generalization. The key requirement is that $\|\delta\|$, $\mathcal{T}$, and $\rho$ in TF-11's ODE share consistent units; TF-00's dimensional analysis verifies this using "surprise" ($-\log P$) as the unit of mismatch.

## Proposition 5.1: Mismatch Inevitability

**Statement.** For any agent-environment pair in $\mathcal{S}_{\text{TFT}}$ (TF-01), the expected squared mismatch is strictly positive whenever at least one of the following holds:

1. Observation noise is non-degenerate: $\mathrm{Var}(o_t \mid \Omega_t) > 0$ on a set of positive probability.
2. One-step predictive mean misspecification is non-zero: $\hat{o}_t = \mathbb{E}[o_t \mid M_{t-1}, a_{t-1}] \neq \bar{o}_t = \mathbb{E}[o_t \mid \Omega_t]$ on a set of positive probability.

Under either condition:

*[Derived]*
$$\mathbb{E}[\|\delta_t\|^2] > 0$$

**Assumptions.** At least one of the two conditions in the statement holds. A practical bridge from TF-03 is:

- $S(M_t) < 1$ implies predictive information is lost relative to the full history.
- To conclude positive one-step squared-error mismatch from this alone requires an additional alignment assumption: the lost predictive information affects the one-step conditional mean under the chosen action conditioning.

Without that alignment assumption, insufficiency still implies positive regret under suitable proper scoring rules, but not necessarily positive one-step mean error.

**Proof sketch.**

1. By TF-01, $H(\Omega_t \mid \mathcal{H}_t) > 0$ — the environment retains residual uncertainty even given the full interaction history.
2. By TF-03, the model generates predictions $\hat{o}_t = \mathbb{E}[o_t \mid M_{t-1}, a_{t-1}]$.
3. Decompose the mismatch into model error and noise. Let $\bar{o}_t = \mathbb{E}[o_t \mid \Omega_t]$ be the true conditional mean. Then:

*[Derived (Proof Step)]*
$$\mathbb{E}[\|\delta_t\|^2] = \mathbb{E}[\|o_t - \hat{o}_t\|^2] = \underbrace{\mathbb{E}[\|\hat{o}_t - \bar{o}_t\|^2]}_{\text{(i) model prediction error}} + \underbrace{\mathbb{E}[\text{Var}(o_t \mid \Omega_t)]}_{\text{(ii) observation noise}}$$

   (This uses an implicit assumption from TF-01's observation model: the observation noise $\varepsilon_t$ at time $t$ is conditionally independent of the agent's prior history $\mathcal{H}_{t-1}$ given the environment state $\Omega_t$. This is the standard "fresh noise" assumption in state-space models — the noise is a property of the observation channel at the moment of observation, not of the agent's past. Given this, the cross-term vanishes by the tower property: condition on $\Omega_t$, under which $\bar{o}_t = \mathbb{E}[o_t \mid \Omega_t]$ is a constant and $\hat{o}_t = \mathbb{E}[o_t \mid M_{t-1}, a_{t-1}]$ is determined by $\mathcal{H}_{t-1}$, so $\bar{o}_t - \hat{o}_t$ is fixed. The remaining factor $\mathbb{E}[o_t - \bar{o}_t \mid \Omega_t] = 0$ by definition of $\bar{o}_t$, giving $\mathbb{E}[(o_t - \bar{o}_t)(\bar{o}_t - \hat{o}_t) \mid \Omega_t] = 0$. Taking outer expectations yields the result. This is orthogonality, not independence — the two terms are uncorrelated but need not be independent.)

4. **Term (ii):** Under condition (1), $\mathbb{E}[\text{Var}(o_t \mid \Omega_t)] > 0$, which alone gives $\mathbb{E}[\|\delta_t\|^2] > 0$.
5. **Term (i):** Under condition (2), $\hat{o}_t \neq \bar{o}_t$ on a set of positive probability, so $\mathbb{E}[\|\hat{o}_t - \bar{o}_t\|^2] > 0$.
6. Either source of positivity suffices; both hold simultaneously in typical settings. $\square$

**Corollary.** Mismatch signals are structurally persistent in realistic TFT regimes. They can be reduced (by improving predictive adequacy, shrinking term (i)) but not eliminated when observation noise is non-degenerate (term (ii)). In deterministic, noiseless, perfectly specified special cases, the mismatch can vanish; TFT treats these as limiting edge cases rather than the typical adaptive regime.

## Why This Is Derived, Not Axiomatic

The proposition above makes precise what was previously stated informally: the mismatch signal follows from TF-03 (the model generates predictions from compressed history) and TF-01 (observations arrive from an uncertain environment). Given any model that predicts and any observation that arrives, their difference exists. The mismatch signal is not an additional assumption but a *consequence* of having a predictive model in an uncertain world.

## Properties

**Vanishing mismatch**: If the model perfectly predicts observations, $\delta_t = 0$ for all $t$. The model needs no updating (but see the ambiguity warning below).

**Decomposition**: The expected squared mismatch decomposes as (from the proof of Proposition 5.1):

*[Derived (from Proposition 5.1)]*
$$\mathbb{E}[\|\delta_t\|^2] = \underbrace{\mathbb{E}[\|\hat{o}_t - \bar{o}_t\|^2]}_{\text{model error (reducible)}} + \underbrace{\mathbb{E}[\text{Var}(o_t \mid \Omega_t)]}_{\text{observation noise (irreducible)}}$$

where $\bar{o}_t = \mathbb{E}[o_t \mid \Omega_t]$. The model can only drive the first term toward zero. The second term — **aleatoric uncertainty** — is a property of the observation channel, not the model. An agent that tries to eliminate all mismatch (including irreducible noise) will **overfit**: adjusting its model to explain noise, degrading future predictions.

**Information content**: The mismatch signal carries information about the gap between model and reality:

*[Discussion — Informational Interpretation]*
$$I(\delta_t; \Omega_t \mid M_{t-1}) > 0 \quad \text{when the model is imperfect}$$

An agent that ignores mismatch signals loses information. An agent that overreacts to them treats noise as signal. The optimal response — the update gain — is the subject of TF-06.

## The Zero-Mismatch Ambiguity

$\delta_t \approx 0$ does **not** necessarily indicate model adequacy. It indicates that predictions match observations, which may occur because:

**(a) Accurate model.** The model genuinely reflects reality. (*Desirable.*)

**(b) Insufficient exploration.** The agent is only observing aspects of the environment that its model already explains, while remaining ignorant of aspects where the model is wrong. (*Dangerous — confirmation bias.*)

**(c) Low channel resolution.** The observation channel is too noisy or low-bandwidth to detect model errors. (*Architectural limitation.*)

Only (a) is desirable. Cases (b) and (c) represent situations where the mismatch signal is **uninformative** — not because the model is good, but because the agent isn't generating observations that would *test* the model.

This ambiguity is why **active testing** — choosing actions specifically to generate informative mismatch signals — can be valuable for agents that have the capacity to vary their actions (TF-07 or TF-08). An agent that only exploits its model (acting to maximize predicted reward) will tend toward case (b). An agent that explores (acting to maximize expected information from $\delta$) will discover model errors faster. When reliable external models are accessible (other agents, reference sources), **query actions** (TF-07 or TF-08) can resolve this ambiguity far more efficiently than direct probing — the external model's response effectively tests the agent's model against a different, potentially superior compression of the same environment.

## Domain Instantiations

| Domain | Mismatch signal $\delta_t$ |
|--------|---------------------------|
| Kalman filter | Innovation: $\tilde{y}_t = y_t - H\hat{x}_{t \mid t-1}$ |
| RL (TD learning) | TD error: $r_t + \gamma \max_{a'} Q(s', a') - Q(s_t, a_t)$ |
| Bayesian inference | Surprise: $-\log P(D_t \mid \theta_{\text{MAP}})$ |
| PID control | Error: $e_t = r_t - y_t$ (setpoint − measurement) |
| Active inference | Free energy gradient: $\nabla_\mu F$ |
| Scientific method | Experimental anomaly: observation contradicting theory |
| Boyd's OODA | Disorientation: outcomes contradicting mental model |
| Immune system | Unrecognized antigen: pathogen not matching antibody repertoire |
