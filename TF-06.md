# TF-06: Action Selection (Derived + Discussion)

Action selection is a **function of the model**, not a separate process. The model's role is not merely to represent the environment but to generate actions, either implicitly or through deliberate computation.

**Epistemic status**: The core claim (action is a function of the model) is *derived* from TF-02. The implicit/explicit distinction and the exploration-exploitation analysis are *discussion* — conceptual properties that follow qualitatively from the formalism but are not formally derived here. The connection to Pearl's causal hierarchy is an *observation* about the structure, not a derivation. A future version may formalize some of these as theorems with proofs; for now they are stated as well-supported properties.

## Action as Model Function

$$a_t = \pi(M_t) \quad \text{(deterministic)}$$
$$a_t \sim \pi(\cdot \mid M_t) \quad \text{(stochastic)}$$

where $\pi$ is the agent's **policy** — the mapping from model state to action. We use $\pi_M$ when emphasizing that the policy is derived from (or embedded in) the model.

This is not a definition imposed on the system but a consequence of TF-02: the model $M_t$ is the agent's compressed history, and the action is a function of what the agent "knows" — i.e., of $M_t$. Any deterministic or stochastic dependence of action on history *through* the model is captured by $\pi(M_t)$.

## Implicit vs. Explicit Action Selection

A critical distinction emerges from the agent's **action fluency** — the degree to which effective action flows from the model without deliberative computation:

**Implicit (model-embedded):** When the model has internalized effective action-selection for the current situation — when $\pi(M_t)$ can be evaluated cheaply — action flows directly from the model state without deliberate computation. This is:

- Boyd's **implicit guidance and control** (the Orient→Act link, bypassing Decide)
- A trained RL policy in exploitation mode
- A well-tuned PID controller
- Expert intuition (Kahneman's System 1)
- A martial artist's trained reflexes
- An organism's instinctive responses
- An organization's established standard procedures

**Explicit (deliberative):** When the model's action-selection mapping is not internalized for the current situation — because the situation is novel, the action space is large, or the stakes demand verification — the agent engages in **internal simulation**, using the model to predict outcomes of candidate actions before selecting one. This is:

- Boyd's explicit **Decide** step
- Monte Carlo Tree Search / planning in RL
- Model Predictive Control's online optimization
- Human deliberate reasoning (Kahneman's System 2)
- An organization's strategic planning process

Deliberation requires *at minimum* **Level 2 epistemic access** (TF-01b) — mental intervention. The agent uses its model to simulate: "what will I observe if I $do(a)$?" across candidate actions, then selects the best. This forward-looking comparison of hypothetical interventions is iterated interventional reasoning: the agent runs the model's causal structure forward under multiple candidate actions and compares the predicted consequences. When the agent additionally reasons retrospectively — "given that I did $a$ and observed $o$, what *would* have happened had I done $a'$ instead?" — it engages Pearl's **Level 3** (counterfactual) access, which is the basis for regret computation, hindsight analysis, and learning from single observations.

In practice, deliberation often involves both levels: comparing candidate actions (Level 2) and evaluating past choices to refine the comparison (Level 3). The computational cost of this simulation — and the mismatch it accumulates during execution (TF-06.5) — is what distinguishes deliberation from implicit action. When the model has internalized the mapping from situation to action (high action fluency), the interventional comparison has been "pre-computed" through prior experience; when it has not, the agent must perform the comparison explicitly, paying the temporal cost.

### Action Fluency vs. Model Sufficiency

Action fluency is distinct from model sufficiency $S(M_t)$ (TF-02). An agent can have high model sufficiency but low action fluency — a chess engine with a perfect model of the rules still requires expensive search to select good moves, because the action space is combinatorially large. Conversely, an agent can have moderate sufficiency but high fluency in a narrow domain — a reflex that responds effectively to the specific situations it evolved for, without representing the environment's full causal structure.

What reflexes, muscle memory, instincts, intuition, trained expertise, and System 1 cognition share is not merely "a cached policy" but that the *action-generating capacity itself* has been absorbed into the model's structure: the model doesn't just *predict* well, it *acts* well, cheaply. The mechanisms by which this absorption occurs vary enormously — evolution (instincts), repetitive training (muscle memory), pattern exposure (intuition), deliberate practice (expertise) — but the result is the same: the computational cost of evaluating $\pi(M_t)$ is low relative to the agent's action timescale.

### The Temporal Advantage of Implicit Action

When two action-selection modes produce equivalent expected outcomes, the faster mode is strictly preferable. This is an instance of the temporal optimality principle (cf. TST T-01: *"For any set of implementations achieving identical outcomes across all non-temporal dimensions, the one requiring least time is optimal"*). Within TFT, TF-06.5 derives this precisely: deliberation of duration $\Delta\tau$ accumulates mismatch cost $\rho \cdot \Delta\tau$, so any deliberation that does not improve action quality is net-harmful.

This creates a **structural pressure toward implicit action**: agents under selective pressure (biological evolution, market competition, adversarial conflict, training optimization) will tend to internalize frequently-needed action-selection patterns, converting explicit deliberation into implicit fluency. The pressure is stronger when:

- **$\rho$ is high** (fast-changing environments penalize deliberation, per TF-06.5)
- **The action pattern recurs frequently** (amortizing the cost of internalization)
- **$\mathcal{T}$ is at the persistence threshold** (no slack for deliberation overhead)

However, this is a *tendency under selection pressure*, not an inevitability. Deliberation remains essential — and may even be the primary mode — when:

- **The situation is genuinely novel** (no internalized pattern applies)
- **The action space is large relative to model capacity** (chess, strategic planning)
- **The stakes are asymmetric** (cost of error vastly exceeds cost of delay)
- **$\rho$ is low** (stable environment allows deliberation without mismatch accumulation)

Whether "Decide" is a separate component of the loop therefore depends on context. In high-tempo domains with recurrent patterns, deliberation tends to shrink as the model absorbs effective action patterns. In domains with vast action spaces or rare high-stakes decisions, deliberation may remain a permanent and central feature of the loop.

## The Exploration-Exploitation Trade-off

Action selection faces a fundamental tension:

**Exploit**: Choose $a_t$ to maximize immediate predicted value given $M_t$:
$$a_t^{\text{exploit}} = \arg\max_a \mathbb{E}[\text{value}(a) \mid M_t]$$

**Explore**: Choose $a_t$ to maximize expected information about model errors:
$$a_t^{\text{explore}} = \arg\max_a \mathbb{E}[I(\delta_{t+1}; \Omega_t \mid M_t, a)]$$

Exploitation uses the model as-is. Exploration tests the model.

The optimal balance depends on:
- **Model uncertainty** ($U_M$ high → explore more)
- **Time horizon** (long → invest in exploration early)
- **Cost of exploration** (high → explore cautiously)
- **Mismatch history** (persistent $\delta \neq 0$ → investigate the source)

This connects directly to the zero-mismatch ambiguity (TF-04): an agent that only exploits will tend toward confirmation bias — observing only what its model already explains. Exploration is the mechanism by which the agent *actively tests* its model, converting the ambiguous case (b) in TF-04 into genuine signal.

## Action as Information Generation

Actions don't merely affect the environment — they **generate information** that passive observation cannot provide. TF-01b establishes three levels of epistemic access grounded in the causal structure of the feedback loop: associational (Level 1), interventional (Level 2), and counterfactual (Level 3).

Action selection is what makes Levels 2 and 3 available. By *choosing* to act and then observing consequences, the agent generates **causal information yield** (TF-01b) — information about how the environment responds to interventions, not merely about correlations. This is why the feedback loop (action → observation → update) is more powerful than passive observation (observation → update) alone.

The exploration-exploitation trade-off (above) is fundamentally about *how much* causal information yield the agent seeks: exploitation maximizes predicted value at Level 1; exploration maximizes expected causal information yield at Level 2.

### Query Actions: Accessing External Models

The discussion above — and the theory's examples throughout — has implicitly framed information-generating actions as *direct environment probes*: do something to the world, observe the result, update the model. But there is a qualitatively different class of actions with distinctive properties: **querying another agent's model**.

When a reliable external model exists — an expert, a database, a reference text, a trustworthy advisor, a well-trained LLM — the action "ask a well-formed question" can yield information equivalent to thousands of probe-observe cycles. The classic illustration: asked to measure a building's height with a barometer, one can drop it from the roof and time the fall, measure shadow ratios, swing it as a pendulum at the top and bottom — all Level 2 probes of the physical environment. Or one can offer the barometer to the building's janitor in exchange for the answer — accessing information that already exists compressed in another agent's model.

**Why query actions are distinctive within the CIY framework:**

**Information density.** A single well-targeted query to a knowledgeable source can carry CIY orders of magnitude higher than any individual environment probe. The source's model has already performed the compression work (TF-02) — extracting the relevant sufficient statistic from a vast interaction history the querying agent has never had. The response transfers the *output* of that compression rather than requiring the agent to reconstruct it from scratch.

**Trust-dependent gain.** The update gain $\eta^*$ for query-derived information depends not on observation channel noise $U_o$ but on the agent's *model of the source's model* — a second-order epistemic state. How much should the agent update on the response? This depends on: the source's domain competence (does their model have high $S$ for the relevant quantity?), their reliability (are they truthful? calibrated?), and their alignment (do they share the agent's objectives, or might they be adversarial?). This connects to game theory and reputation as a meta-model — the agent needs a model of *which sources to trust how much about what*.

**Pre-compressed information.** Environmental probes return raw observations that must be compressed through the agent's own model. Query responses arrive *already compressed* in the source's representational framework. This is why they're so information-dense — but it also introduces a translation cost when the source's representation doesn't align with the agent's. An expert's answer may be incomprehensible to a novice not because the information is absent but because the agent lacks the representational capacity (TF-07's model class $\mathcal{M}$) to absorb it.

**Structural adaptation via external models.** Query actions can trigger not just parametric updates but structural change. Encountering another agent's model — through conversation, reading, apprenticeship, or consultation — is one of the primary mechanisms by which agents acquire new representational frameworks. This connects to TF-07's "grafting" mechanism: incorporating external structure rather than building it de novo. Boyd's thought experiment specifically illustrates cross-domain recombination — freeing components from their native domains and reassembling them into novel model structures.

**Implications for optimal action selection.** When high-CIY query channels are available, the unified policy objective (below) will tend to favor query actions over direct probes, particularly when:

- The agent's own $U_M$ is high (it has much to learn)
- A trusted source with high $S$ for the relevant domain is accessible
- The cost of querying (social, monetary, time) is low relative to the cost of direct probing
- The information needed is about *structure* (requiring many probes to reconstruct) rather than about the agent's *specific situation* (which only the agent can observe)

Direct probing remains essential when the information needed is situational (no external model has access to the agent's specific environment state), when no trustworthy source exists, or when the agent needs to verify claims rather than accept them. The optimal agent uses both channels, allocating actions to whichever has higher expected CIY per unit cost.

### The Adversarial Mirror: Deception and Model Corruption

Query actions have a dark mirror in adversarial settings. The same communicative channel that enables cooperative information transfer — where one agent's response improves another's model — can be exploited to *degrade* the opponent's model. This is the domain of game theory, information warfare, and strategic communication, and it connects directly to TF-08's adversarial dynamics.

**Deception as negative CIY.** If a cooperative query yields positive CIY (the response genuinely improves the agent's model), a deceptive response yields what might be called *negative effective CIY*: the agent updates its model in a direction that *increases* rather than decreases model-reality mismatch. The update gain $\eta^*$ for the victim depends on trust in the source; a successful deception exploits high trust to inject a large, misdirected update. In the Lyapunov framework (Appendix A), this is an adversary directly contributing to $\rho_B$ — not by changing B's physical environment, but by corrupting B's *model* of that environment.

**Active OODA loop interference.** A central theme in Boyd's work — and in strategic thought at least since Sun Tzu — is that an adversary can do more than merely adapt faster (TF-08); it can *actively interfere* with the opponent's feedback loop: generating ambiguous or contradictory signals to degrade the opponent's Orient phase, creating false patterns to induce model errors, manipulating the information environment to increase the opponent's $\rho$ while providing no genuine signal. Feints, disinformation, electronic warfare, strategic ambiguity, and propaganda are all instances of one agent using communicative or signal-manipulating actions to drive another agent's mismatch upward — the adversarial coupling $\gamma_A$ in Appendix A's Proposition A.3 operating through the information channel rather than through physical action.

**Trust as a meta-model under adversarial pressure.** In adversarial settings, the agent's model of source reliability becomes a critical adaptive target in its own right. An adversary who can corrupt this meta-model — making the victim trust unreliable sources or distrust reliable ones — achieves a second-order attack: not just injecting bad information, but compromising the victim's *capacity to evaluate information*. This connects to the effects spiral (Corollary A.3.1): once an agent's trust calibration degrades, it becomes increasingly vulnerable to further deception, creating a positive-feedback loop in model corruption.

**Symmetry with cooperative query actions.** The same formal structure — communicative actions with trust-dependent gain operating on another agent's model — encompasses both the cooperative case (teaching, consulting, honest signaling) and the adversarial case (deception, disinformation, strategic ambiguity). What differs is alignment: whether the source's response is optimized to improve or to degrade the receiver's model. The game-theoretic literature on cheap talk, signaling games, and mechanism design addresses when honest communication is incentive-compatible — a question TFT does not attempt to answer but whose relevance it makes formally precise.

## Toward a Unified Policy Objective

The exploration-exploitation tension suggests a single objective that the optimal policy $\pi^*$ maximizes:

$$\pi^*(M_t) = \arg\max_a \left[\mathbb{E}[\text{value}(a) \mid M_t] + \lambda(M_t) \cdot \mathbb{E}[\text{CIY}(a) \mid M_t]\right]$$

The first term is the exploitation objective — expected value given the current model. The second term is the exploration objective — expected causal information yield of the action (TF-01b). The coefficient $\lambda(M_t)$ controls the balance and should depend on the agent's epistemic state:

- When $U_M$ is high (model uncertain): $\lambda$ is large — exploration is valuable because the model has much to learn.
- When $U_M$ is low (model confident): $\lambda$ is small — exploitation dominates because the model is (probably) already good.
- When the time horizon is long: $\lambda$ should be larger — the information gained now compounds over many future decisions.
- When $\rho$ is high (fast-changing environment): $\lambda$ should be larger — the model is perpetually uncertain because the environment keeps changing (connecting to TF-08).

**Epistemic status.** This formalization is *discussion-grade*: structurally correct but not yet derived from first principles within TFT. The specific form of $\lambda(M_t)$ is a meta-parameter whose optimal value is situation-dependent. The structural claim — that the optimal policy jointly maximizes value and causal information — is well-supported by convergent results in Bayesian RL (Bayes-optimal exploration), active inference (expected free energy minimization), and information-directed sampling.

**Connection to active inference.** The Free Energy Principle's "expected free energy" objective decomposes into an extrinsic value term (pragmatic, goal-directed) and an epistemic value term (information-seeking). The TFT formulation above is structurally isomorphic: expected value ≈ extrinsic value, expected CIY ≈ epistemic value. Whether this convergence is deep (both are instances of the same mathematical principle) or superficial (similar-looking objectives from different foundations) is an open question. The TFT formulation has the advantage of grounding exploration in explicitly *causal* information rather than in entropy reduction, which may be more precise — not all uncertainty reduction is equally valuable; causal information is specifically what enables better *intervention*, not merely better *prediction*.

## Domain Instantiations

| Domain | Implicit action | Explicit deliberation | Exploration: direct probing | Exploration: query actions |
|--------|----------------|----------------------|---------------------------|--------------------------|
| Kalman + LQR | LQR control law from $\hat{x}_t$ | — (separation principle) | Dual control (rare) | — (no external models) |
| RL | Greedy policy $\arg\max Q(s,a)$ | MCTS, planning, rollouts | ε-greedy, UCB, Thompson | Imitation learning, reward shaping from demonstrations |
| PID | $u = K_p e + K_i \int e + K_d \dot{e}$ | — (no deliberation) | Perturbation testing | Consulting plant specifications |
| Boyd's OODA | IG&C (Orient→Act) | Explicit Decide step | Probing, feints, recon | Intelligence gathering, interrogation, liaison with allies |
| Organism | Reflexes, habits | Deliberate planning | Play, curiosity, foraging | Social learning, asking, observing experts |
| Organization | Standard procedures | Strategic planning | R&D, experiments, pilots | Hiring consultants, benchmarking, acquiring companies |
| Science | Apply known theory | Model-based prediction | Experimentation | Literature review, peer consultation, conference attendance |
| Immune | Innate response | — (no deliberation) | Random antibody generation | Maternal antibodies, microbiome signaling |
