# TF-06: Action Selection (Derived + Discussion)

Action selection is a **function of the model**, not a separate process. The model's role is not merely to represent the environment but to generate actions, either implicitly or through deliberate computation.

**Epistemic status**: The core claim (action is a function of the model) is *derived* from TF-02. The implicit/explicit distinction and the exploration-exploitation analysis are *discussion* — conceptual properties that follow qualitatively from the formalism but are not formally derived here. The connection to Pearl's causal hierarchy is an *observation* about the structure, not a derivation. A future version may formalize some of these as theorems with proofs; for now they are stated as well-supported properties.

## Action as Model Function

$$a_t = \pi(M_t) \quad \text{(deterministic)}$$
$$a_t \sim \pi(\cdot \mid M_t) \quad \text{(stochastic)}$$

where $\pi$ is the agent's **policy** — the mapping from model state to action. We use $\pi_M$ when emphasizing that the policy is derived from (or embedded in) the model.

This is not a definition imposed on the system but a consequence of TF-02: the model $M_t$ is the agent's compressed history, and the action is a function of what the agent "knows" — i.e., of $M_t$. Any deterministic or stochastic dependence of action on history *through* the model is captured by $\pi(M_t)$.

## Implicit vs. Explicit Action Selection

A critical distinction emerges from the quality of the model:

**Implicit (model-embedded):** When the model is adequate ($S(M_t) \approx 1$ per TF-02) and the mapping from model state to effective action has been learned, $\pi(M_t)$ executes directly without deliberate computation. This is:

- Boyd's **implicit guidance and control** (the Orient→Act link, bypassing Decide)
- A trained RL policy in exploitation mode
- A well-tuned PID controller
- Expert intuition (Kahneman's System 1)
- A martial artist's trained reflexes

**Explicit (deliberative):** When the model is uncertain, the situation is novel, or the stakes are high, the agent engages in **internal simulation** — using the model to predict outcomes of candidate actions before selecting one. This is:

- Boyd's explicit **Decide** step
- Monte Carlo Tree Search / planning in RL
- Model Predictive Control's online optimization
- Human deliberate reasoning (Kahneman's System 2)
- An organization's strategic planning process

The key insight: **explicit deliberation is the fallback, not the default.** The goal of learning and model improvement is to make deliberation unnecessary — to build a model from which effective action flows implicitly. Deliberation is computationally expensive and slow; implicit action is fast and cheap. An agent that must deliberate on every action cannot operate at high tempo.

This resolves the question of whether "Decide" is a separate component of the loop: in general, no. It is a mode of action selection that occurs when the model alone is insufficient. As the model improves, Decide shrinks. In the limit of a perfect model, all action is implicit.

## The Exploration-Exploitation Trade-off

Action selection faces a fundamental tension:

**Exploit**: Choose $a_t$ to maximize immediate predicted value given $M_t$:
$$a_t^{\text{exploit}} = \arg\max_a \mathbb{E}[\text{value}(a) \mid M_t]$$

**Explore**: Choose $a_t$ to maximize expected information about model errors:
$$a_t^{\text{explore}} = \arg\max_a \mathbb{E}[I(\delta_{t+1}; \Omega_t \mid M_t, a)]$$

Exploitation uses the model as-is. Exploration tests the model.

The optimal balance depends on:
- **Model uncertainty** ($U_M$ high → explore more)
- **Time horizon** (long → invest in exploration early)
- **Cost of exploration** (high → explore cautiously)
- **Mismatch history** (persistent $\delta \neq 0$ → investigate the source)

This connects directly to the zero-mismatch ambiguity (TF-04): an agent that only exploits will tend toward confirmation bias — observing only what its model already explains. Exploration is the mechanism by which the agent *actively tests* its model, converting the ambiguous case (b) in TF-04 into genuine signal.

## Action as Information Generation

Actions don't merely affect the environment — they **generate information** that passive observation cannot provide. TF-01b establishes three levels of epistemic access grounded in the causal structure of the feedback loop: associational (Level 1), interventional (Level 2), and counterfactual (Level 3).

Action selection is what makes Levels 2 and 3 available. By *choosing* to act and then observing consequences, the agent generates **causal information yield** (TF-01b) — information about how the environment responds to interventions, not merely about correlations. This is why the feedback loop (action → observation → update) is more powerful than passive observation (observation → update) alone.

The exploration-exploitation trade-off (above) is fundamentally about *how much* causal information yield the agent seeks: exploitation maximizes predicted value at Level 1; exploration maximizes expected causal information yield at Level 2.

## Domain Instantiations

| Domain | Implicit action | Explicit deliberation | Exploration mechanism |
|--------|----------------|----------------------|----------------------|
| Kalman + LQR | LQR control law from $\hat{x}_t$ | — (separation principle) | Dual control (rare) |
| RL | Greedy policy $\arg\max Q(s,a)$ | MCTS, planning, rollouts | ε-greedy, UCB, Thompson |
| PID | $u = K_p e + K_i \int e + K_d \dot{e}$ | — (no deliberation) | Perturbation testing |
| Boyd's OODA | IG&C (Orient→Act) | Explicit Decide step | Probing, feints, recon |
| Organism | Reflexes, habits | Deliberate planning | Play, curiosity, foraging |
| Organization | Standard procedures | Strategic planning | R&D, experiments, pilots |
| Immune | Innate response | — (no deliberation) | Random antibody generation |
