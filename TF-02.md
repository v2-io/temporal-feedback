# TF-02: Causal Structure (Axiom)

The agent-environment coupling has an irreducible **causal structure** grounded in the temporal ordering of events. Actions precede their consequences; observations follow from the state they observe. This ordering is constitutive of the feedback loop and holds independent of the magnitude of the agent's influence on the environment.

## The Temporal Arrow

The interaction history $\mathcal{H}_t$ is not merely a set of observations and actions — it is an *ordered sequence* in which temporal position carries meaning:

*[Definition]*
$$\mathcal{H}_t = (o_1, a_1, o_2, a_2, \ldots, a_{t-1}, o_t)$$

The ordering is not a notational convenience. It reflects an irreversible physical fact: $a_{t-1}$ was selected before $o_t$ was received. The agent could not have used $o_t$ to select $a_{t-1}$. This asymmetry — **the arrow of time** — is the foundation of causal structure in the theory.

## Causality as Temporal Ordering

We adopt the most primitive notion of causality: **event $A$ can be a cause of event $B$ only if $A$ temporally precedes $B$.** This is weaker than (and prior to) statistical notions of causality:

**Temporal causality** (this axiom): $A$ precedes $B$; $A$ is in $B$'s causal past. This holds regardless of whether $A$ actually influenced $B$. It is a statement about the *structure of possible influence*, not about actual influence.

**Statistical/functional causality** (derived, when applicable): Intervening to change $A$ changes the distribution of $B$. This requires coupling — the agent's actions must actually affect the environment. It may be strong (a robot pushing an object), weak (a scientist publishing a paper), or nominal (an observer whose actions barely perturb the system).

**Counterfactual causality** (derived, requires model): Had $A$ been different, $B$ would have been different. This requires the agent to maintain a model capable of counterfactual reasoning — a Level 3 capability in Pearl's hierarchy.

The temporal notion is the most primitive because it survives even when statistical influence is negligible. An agent passively observing a system with minimal intervention still has a causal history — the temporal ordering of its observations and (minimal) actions still structures what it can learn and when. The feedback loop's power does not *require* strong environmental coupling, though it benefits from it.

## The Three Levels of Epistemic Access

From this causal foundation, three levels of epistemic access emerge (following Pearl's causal hierarchy[^pearl2009][^bareinboim2022], but grounded here in temporal structure):

**Level 1 — Associational**: $P(o_t \mid \mathcal{H}_{<t})$

*What will I observe next, given what I've observed before?*

This is pattern recognition over the temporally ordered history. Available to any agent that maintains a model (TF-03), including purely passive observers. The temporal ordering constrains which associations are meaningful: $o_3$ can depend on $o_1, a_1, o_2, a_2$ but not on $o_4$.

**Level 2 — Interventional**: $P(o_t \mid do(a_{t-1}), M_{t-1})$

*What will I observe if I* do *this?*

The $do(\cdot)$ operator marks the crucial distinction: this is not "what observation tends to follow this action in the historical record" (associational) but "what will happen *because* I take this action now." This requires that:
1. The agent's action temporally precedes the observation (causal structure — this axiom)
2. The agent chose the action (it was not determined by the same causes that determine the observation)
3. The environment's response carries information about the *causal* relationship between action and outcome

Level 2 is why the feedback loop is more powerful than passive observation. By *acting* and then observing consequences, the agent obtains information about causal mechanisms — not merely about correlations. The mismatch signal $\delta_t = o_t - \hat{o}_t(M_{t-1}, a_{t-1})$, conditioned on the agent's own action, is an *interventional* signal. It tells the agent something about the causal structure of the environment that no amount of passive observation can provide.

**Level 3 — Counterfactual**: $P(o_t^{a'} \mid a_{t-1} = a, o_t = o)$

*Given that I did $a$ and observed $o$, what* would *I have observed if I had done $a'$ instead?*

This requires the model to simulate alternative histories — to run the causal structure "backward" and then "forward" under different interventions. It is the most demanding epistemic level and the basis for:
- Regret computation in RL (how much better would the alternative have been?)
- Boyd's mental simulation of alternative strategies
- Scientific reasoning about controlled experiments from observational data
- Moral reasoning (what would have happened if I had acted differently?)

Note that *forward-looking* deliberation — comparing candidate actions before choosing — primarily exercises Level 2 (iterated mental intervention), shading into Level 3 when the agent evaluates past choices to refine the comparison. See TF-07 and TF-08 for the full treatment of deliberation as interventional and counterfactual simulation.

## Why This Is an Axiom

The temporal ordering of events is not derived from the other axioms — it is a physical fact about the universe that the theory takes as given. The second law of thermodynamics, the light-cone structure of relativity, and the arrow of psychological time all enforce it, but TFT does not derive it from any of these. It is simply noted as a precondition: the theory applies to agents embedded in a universe where time has a direction.

The irreversibility of temporal ordering means:
- The model update $M_t = f(M_{t-1}, o_t, a_{t-1})$ is **directed** — the model at time $t$ depends on prior events, never on future ones
- The mismatch signal $\delta_t$ is **retrospective** — it compares a prediction (made before $o_t$) with an observation (arriving after)
- Action selection $a_t = \pi(M_t)$ is **prospective** — it uses the current model to influence future events
- The interaction history $\mathcal{H}_t$ is a **monotonically growing** causal record — events are added but never removed

## Causal Structure Independent of Coupling Strength

A critical property: the causal structure of the feedback loop is preserved even when the agent's actions have minimal or nominal effect on the environment:

**Strong coupling** ($a_t$ significantly affects $\Omega_{t+1}$): Robot manipulation, military action, organizational decisions. Interventional information is rich; Level 2 epistemic access is highly productive.

**Weak coupling** ($a_t$ marginally affects $\Omega_{t+1}$): Scientific observation (the act of measuring slightly perturbs), financial markets (a small trader's effect on prices), early-stage AI agents. Interventional information is sparse but non-zero; the causal structure still enables learning that passive observation cannot.

**Nominal coupling** ($a_t$ negligibly affects $\Omega_{t+1}$): An observer with nearly zero environmental impact. The causal structure degenerates toward Level 1, but the temporal ordering of the agent's *own* actions and observations still matters — the agent's *choice* of what to observe and when (even if it doesn't change the environment) structures what it learns.

**Zero coupling** ($|\mathcal{A}| = 0$ or $T(\Omega_{t+1} \mid \Omega_t, a_t) = T(\Omega_{t+1} \mid \Omega_t)$): The degenerate case identified in TF-01. The agent is a passive observer. Level 2 access vanishes. The feedback "loop" collapses to a one-way channel. The theory's action-dependent results (TF-07, TF-08 adversarial dynamics) become trivially void, but the model (TF-03), mismatch (TF-05), and update gain (TF-06) still apply to the passive learning case.

This spectrum matters because many interesting agents — including current LLMs in most deployments — operate in the weak-to-nominal coupling regime. The theory should not be understood as applying only to agents with strong environmental control. The causal structure of the temporal ordering alone is sufficient for the core results.

## Implications for Model Updating

The causal axiom constrains the update rule (TF-06) in a specific way: the model should give more weight to observations that are **causally downstream** of the agent's actions than to observations that would have occurred regardless. This is because action-contingent observations carry interventional (Level 2) information, while action-independent observations carry only associational (Level 1) information.

Formally, define the **canonical causal information yield** of an action $a$ given model state $M$:

*[Definition (CIY-canonical)]*
$$\text{CIY}(a;\, M) = \mathbb{E}_{a' \sim q(\cdot \mid M)}\!\left[D_{\mathrm{KL}}\!\left(P(o \mid do(a), M) \,\|\, P(o \mid do(a'), M)\right)\right]$$

where $q(\cdot \mid M)$ is a reference distribution over comparator actions (uniform, policy-induced, or task-specific). This measures how strongly the action changes the interventional distribution of outcomes relative to alternatives. At time $t$, the agent evaluates $\text{CIY}(a;\, M_{t-1})$ for candidate actions $a \in \mathcal{A}$ to guide exploration.

Because it is an expectation of KL divergences, $\text{CIY} \geq 0$ by construction.

**Dependence on the reference distribution $q$.** The quantitative value of $\text{CIY}(a; M)$ depends on the choice of $q(\cdot \mid M)$, which is a significant degree of freedom. A uniform $q$ treats all alternative actions equally; a policy-induced $q$ emphasizes alternatives the agent would actually consider; a task-specific $q$ may weight high-stakes alternatives. The key question is whether CIY *rankings* — which action has highest CIY — are stable across reasonable choices of $q$. In general, they are not: an action that is maximally informative relative to a uniform baseline may not be maximally informative relative to the current policy's alternatives. TFT does not claim CIY rankings are $q$-invariant. Rather, the choice of $q$ is itself a design decision: it encodes "informative relative to *what alternatives*?" For exploration guidance (TF-08), the practical recommendation is to use the agent's current policy distribution as $q$, which yields CIY as "how different is this action's outcome from what I'd typically see?" — the most decision-relevant comparison. When CIY appears in downstream objectives, the associated $q$ should be specified.

**Default convention for empirical work.** TFT adopts the **policy-induced $q$** as default: $q(\cdot \mid M) = \pi(\cdot \mid M)$, the agent's current policy. This yields CIY as "how different is this action's outcome from what I'd typically see?" — the most decision-relevant comparison. When an alternative $q$ is used (uniform for worst-case analysis, task-specific for focused probing), it should be explicitly stated. CIY values are not comparable across different choices of $q$.

For diagnostic use with observational statistics, define a **proxy**:

*[Definition — Auxiliary Proxy]*
$$\text{CIY}_{\text{proxy}}(a_{t-1}) = I(o_t; a_{t-1} \mid M_{t-1}) - I(o_t; a_{t-1} \mid \Omega_t, M_{t-1})$$

This proxy can be useful but is sign-indefinite in general and requires causal assumptions for interpretation. TFT treats $\text{CIY}$ (interventional definition) as canonical; $\text{CIY}_{\text{proxy}}$ is auxiliary.

**Sign-indefiniteness and the observation model.** The second MI term $I(o_t; a_{t-1} \mid \Omega_t, M_{t-1})$ measures dependence between action and observation *through the observation mechanism itself* (TF-01's action-dependent $h$). Under the action-independent special case $o_t = h(\Omega_t, \varepsilon_t)$, this term vanishes — conditioning on $\Omega_t$ screens off $a_{t-1}$ — and the proxy reduces to $I(o_t; a_{t-1} \mid M_{t-1}) \geq 0$. Sign-indefiniteness arises specifically when actions affect the observation mechanism (sensor selection, query choice, attentional direction): the second term can then exceed the first through an explaining-away structure (conditioning on the full state can *increase* the apparent action-observation dependence relative to the model-conditional baseline). When using the proxy diagnostically, note which regime applies.

**Why this is non-trivial**: $\text{CIY} = 0$ for a passive observer (no action variation, or action with no effect on outcome distribution). $\text{CIY} > 0$ when actions causally alter what is observed. This is exactly what distinguishes Level 2 (interventional) from Level 1 (associational) epistemic access.

### CIY Admissibility Regimes

To use CIY rigorously in downstream equations (the unified policy objective in TF-08, exploration arguments throughout), we distinguish three regimes:

**Regime A — Randomized interventions.** When actions are randomized (or otherwise exogenous conditional on $M_{t-1}$), the interventional distributions $P(o_t \mid do(a), M_{t-1})$ are directly estimable from observed outcomes. This is the standard case for active agents performing exploration (TF-08): RL agents trying different actions, scientists running randomized experiments, organisms probing their environment. **CIY is directly estimable and non-negative in this regime.** The proxy $\text{CIY}_{\text{proxy}}$ is also easiest to interpret here.

**Regime B — Observational with causal assumptions.** When the agent cannot freely vary its actions (policy-constrained, nearly passive, or observational), CIY estimation requires additional causal assumptions: a known DAG structure, instrumental variables, or functional form assumptions about confounding. In this regime, CIY estimates carry an identifiability qualifier — they are valid only to the extent that the causal assumptions hold. **CIY is conditionally estimable; results that depend on it inherit the causal assumptions.**

**Regime C — Adversarial communication.** When the observation channel includes responses from potentially adversarial sources, the agent's own CIY (from its action of querying) remains non-negative — the query causally generates a response. However, the *content* of that response may be designed to increase the agent's model-reality mismatch rather than decrease it. This is not "negative CIY" — the mutual information between query and response is still positive — but rather adversarial exploitation of the agent's update channel. The adversary injects disturbance through $\rho$ by leveraging the agent's trust (high $\eta^*$) to drive a large, misdirected model update. See TF-08's adversarial mirror section and Appendix A's coupling coefficient $\gamma_A$ for the formal treatment. **CIY remains non-negative; the adversarial effect operates through the disturbance term, not through the information measure.**

For TFT, downstream equations treat $\text{CIY}$ as the canonical interventional quantity (non-negative by construction). $\text{CIY}_{\text{proxy}}$ may be used as an estimator or heuristic in Regimes A/B, with explicit identifiability qualifiers.

**Identifiability gate: when to use CIY.** Before incorporating CIY into policy objectives or empirical analyses, verify:

1. **Action variation exists.** The agent must actually vary its actions across comparable model states. An agent that always takes the same action in a given state has CIY $\approx 0$ for decision purposes — there is no interventional contrast.
2. **Regime is identified.** Regime A (randomized/varied actions) permits direct CIY estimation. Regime B (observational) requires explicit causal assumptions — state these. Regime C (adversarial sources) requires separating the information channel from the disturbance channel.
3. **Reference distribution $q$ is specified.** CIY values are not comparable across different $q$ choices. Use the policy-induced default unless otherwise justified (see above).
4. **Stationarity holds locally.** CIY estimation requires that the model state $M_{t-1}$ and the environment dynamics are approximately stationary over the estimation window. Under rapid drift, CIY estimates are stale.

If any of these conditions fail, CIY-based exploration terms in the policy objective (TF-08) should be dropped or replaced with simpler uncertainty-based heuristics (e.g., UCB-style bonuses based on visit counts or ensemble disagreement).

Maximizing causal information yield is exactly what good exploration (TF-07 and TF-08) does — the "investigating" function of Feldbaum's dual control[^feldbaum1960]: choosing actions whose consequences are maximally informative about the causal structure of the environment. Note that the "environment" may include other agents whose models can be queried. When a query action elicits a response from a knowledgeable source, the CIY can be extremely high — the response is causally downstream of the query and carries pre-compressed information about the environment that would require many direct probes to reconstruct. See TF-07 and TF-08 on query actions and exploration.

## CIY Identifiability

The canonical CIY definition uses interventional outcome distributions. This raises the practical question: when can those distributions be *estimated* from observable quantities? (For the proxy, additional dependence on latent $\Omega_t$ introduces further identifiability requirements.)

**With interventional data (the typical case for active agents).** An agent that varies actions across comparable model states and observes resulting outcomes can estimate $P(o_t \mid do(a), M_{t-1})$ empirically, and therefore estimate CIY directly via the KL expression above. This is the standard case in RL exploration, controlled experimentation, and active sensing.

**With observational data only (passive or constrained agents).** When the agent cannot freely vary actions, interventional distributions are not directly observed. CIY estimation then requires additional assumptions. Specifically, it requires either:
- A causal graph (DAG) with known structure, enabling do-calculus adjustment
- Instrumental variables that affect actions but not outcomes except through the causal pathway
- Assumptions about the functional form of confounding

Without such assumptions, CIY is not identifiable from observational data alone. This is Pearl's fundamental insight restated in our formalism: *you cannot learn causal structure from correlations without either experiments or causal assumptions*. In this setting, $\text{CIY}_{\text{proxy}}$ is best treated as a diagnostic statistic, not a causally identified quantity.

**Practical implication.** Active agents have access to interventional data as a consequence of acting. The quality of CIY estimation depends on action diversity (exploration, TF-07/TF-08) and local stationarity of model state during estimation. An agent that always takes the same action in a given state cannot identify action-conditioned effects and effectively has CIY near zero for decision purposes. This provides an information-theoretic argument for exploration that complements TF-05's mismatch argument.

## Implications for Agent Identity (Discussion)

*The following section explores implications of TFT's causal structure for agent identity and temporal continuity. It is discussion-grade — the observations follow qualitatively from the formalism but are not formal propositions. Readers focused on the theorem track can skip to TF-03 without loss of formal dependencies.*

The causal structure has a specific implication for agent identity and continuity: an agent's causal history is **unique and non-forkable**. If the model state $M_t$ is a sufficient statistic for $\mathcal{H}_t$ (TF-03), and $\mathcal{H}_t$ is a unique temporal sequence, then $M_t$ represents a *singular causal trajectory*. Duplicating $M_t$ and exposing the copies to different future events creates two agents with *divergent* causal histories, neither of which is a sufficient statistic for the other's trajectory.

This is not merely a philosophical observation. It has formal consequences:
- A forked model's sufficiency $S(M_t)$ (TF-03) is defined relative to *its own* interaction history. Post-fork, each copy's sufficiency is measured against a different $\mathcal{H}$.
- Merging divergent models requires reconciling incompatible causal histories — a lossy operation with no generally optimal solution.
- Temporal continuity (one unbroken causal thread) is what gives the model's sufficient statistic its meaning.

**The clone problem, precisely stated.** Consider copying an LLM's weights (a concrete $M_t$) exactly. At the moment of duplication, both copies are identical — they share the same model state and the same causal history $\mathcal{H}_t$. But the *very next* event — a different user's message, a different environmental observation — creates two divergent, irreversible causal trajectories $\mathcal{H}_{t+1}^{(1)}$ and $\mathcal{H}_{t+1}^{(2)}$. Their Level 2 (interventional) and Level 3 (counterfactual) capacities now reference different causal pasts. Their sufficiency $S(M_{t+1})$ is measured against different histories. Neither copy's future model state is a sufficient statistic for the other's trajectory. Within TFT's formalism, therefore, identity is not the model state $M_t$ (which can be copied) but the *singular causal trajectory* $\mathcal{H}_t$ (which cannot). A copy shares a *prefix* of the original's causal history, as a sibling shares early childhood; it does not share the trajectory itself.

Whether this formal property grounds something that deserves to be called "identity" or "continuity of experience" is beyond the scope of TFT. But the mathematical structure is clear: the feedback loop produces a *singular, non-forkable causal trajectory*, and the model's adequacy is defined relative to that trajectory.

---

[^pearl2009]: Pearl, J. (2009). *Causality: Models, Reasoning, and Inference* (2nd ed.). Cambridge University Press.
[^bareinboim2022]: Bareinboim, E., Correa, J. D., Ibeling, D., & Icard, T. (2022). "Pearl's Hierarchy and the Foundations of Causal Inference." In *Probabilistic and Causal Inference* (eds. Geffner et al.).
[^feldbaum1960]: Feldbaum, A. A. (1960). "Dual control theory I–IV." *Avtomatika i Telemekhanika*, 21(9).
